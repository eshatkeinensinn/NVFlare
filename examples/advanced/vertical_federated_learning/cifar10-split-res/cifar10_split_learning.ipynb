{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cada310b-e776-4b9a-aabe-f111c31efcc2",
   "metadata": {},
   "source": [
    "# Split Learning with CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a00eb-cabe-4461-aa95-639ac8229d47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install the required packages for training in the current Jupyter kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a6dcba-9570-4ddf-bc25-b60a5f2a9adc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvflare>=2.4.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: torch in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 3)) (0.16.2)\n",
      "Requirement already satisfied: tensorboard in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 4)) (2.15.1)\n",
      "Requirement already satisfied: openmined.psi==1.1.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: pandas in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from -r ./requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from openmined.psi==1.1.1->-r ./requirements.txt (line 5)) (3.20.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (42.0.0)\n",
      "Requirement already satisfied: Flask==2.2.5 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.2.5)\n",
      "Requirement already satisfied: Werkzeug==2.2.2 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: Flask-JWT-Extended==4.4.3 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (4.4.3)\n",
      "Requirement already satisfied: Flask-SQLAlchemy==2.5.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.31 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.4.31)\n",
      "Requirement already satisfied: grpcio==1.51.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.51.1)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: numpy in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.9.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.28.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: msgpack>=1.0.3 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: docker>=6.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (12.0)\n",
      "Requirement already satisfied: pyhocon in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvflare>=2.4.0->-r ./requirements.txt (line 1)) (0.3.60)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from Flask==2.2.5->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from Flask==2.2.5->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from Flask==2.2.5->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: PyJWT<3.0,>=2.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from Flask-JWT-Extended==4.4.3->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from SQLAlchemy==1.4.31->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from Werkzeug==2.2.2->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.1.4)\n",
      "Requirement already satisfied: filelock in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torch->-r ./requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r ./requirements.txt (line 2)) (12.3.101)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from torchvision->-r ./requirements.txt (line 3)) (10.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (3.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (69.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from tensorboard->-r ./requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from pandas->-r ./requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from pandas->-r ./requirements.txt (line 6)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from pandas->-r ./requirements.txt (line 6)) (2023.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from cryptography>=36.0.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from docker>=6.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from docker>=6.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./requirements.txt (line 4)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r ./requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from requests>=2.28.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from requests>=2.28.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from requests>=2.28.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: pyparsing<4,>=2 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from pyhocon->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from sympy->torch->-r ./requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->nvflare>=2.4.0->-r ./requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r ./requirements.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r ./requirements.txt (line 4)) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef76950-e43e-4d99-8f66-749ed6078a3a",
   "metadata": {},
   "source": [
    "Set `PYTHONPATH` to include custom files of this example and some reused files from the [CIFAR-10](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/cifar10) examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f85872-b039-4f77-b381-ff06b2503b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"cifar10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d95481-b7fc-49ff-ad50-5dbad398db6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from splitnn.cifar10_vertical_data_splitter import Cifar10VerticalDataSplitter\n",
    "except ImportError as e:\n",
    "     raise ImportError(\"PYTHONPATH is not set properly\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653cbf2-92f2-4a22-8317-69cfb0266e92",
   "metadata": {},
   "source": [
    "## 1. Download and split the CIFAR-10 dataset\n",
    "To simulate a vertical split dataset, we first download the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and distribute it between the two clients, assuming an `OVERLAP` of 10,000 samples between the two clients' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4130b15-09e6-456f-a3c7-87c8ee9e07f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:Cifar10VerticalDataSplitter:[identity=local, run=_]: Partition CIFAR-10 dataset into vertically with 10000 overlapping samples.\n",
      "Files already downloaded and verified\n",
      "INFO:Cifar10VerticalDataSplitter:[identity=local, run=_]: save /tmp/cifar10_vert_splits/overlap.npy\n",
      "INFO:Cifar10VerticalDataSplitter:[identity=local, run=_]: save /tmp/cifar10_vert_splits/site-1.npy\n",
      "INFO:Cifar10VerticalDataSplitter:[identity=local, run=_]: save /tmp/cifar10_vert_splits/site-2.npy\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DIR = \"/tmp/cifar10_vert_splits\"\n",
    "OVERLAP = \"10000\"\n",
    "%run ./cifar10_split_data_vertical.py --split_dir $SPLIT_DIR --overlap $OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af257e69-2bb7-49b6-ac6c-f007b0e6618e",
   "metadata": {},
   "source": [
    "## 2. Run private set intersection\n",
    "We are using NVFlare's FL simulator to run the following experiments.\n",
    "\n",
    "In order to find the overlapping data indices between the different clients participating in split learning, \n",
    "we randomly select an subset of the training indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb7290a-48ff-4e80-be58-5e6b0e0f9379",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:MPM:=========== MPM: started to run forever\n",
      "2024-03-25 13:19:35,509 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2024-03-25 13:19:35,514 - CoreCell - INFO - server: creating listener on tcp://0:47335\n",
      "2024-03-25 13:19:35,534 - CoreCell - INFO - server: created backbone external listener for tcp://0:47335\n",
      "2024-03-25 13:19:35,536 - ConnectorManager - INFO - 4011917: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2024-03-25 13:19:35,539 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:25770] is starting\n",
      "2024-03-25 13:19:36,044 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:25770\n",
      "2024-03-25 13:19:36,046 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:47335] is starting\n",
      "2024-03-25 13:19:36,189 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 60799\n",
      "2024-03-25 13:19:36,190 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2024-03-25 13:19:36,201 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2024-03-25 13:19:36,212 - ClientManager - INFO - Client: New client site-1@100.122.101.102 joined. Sent token: be1914b2-515f-42e2-b7bd-7cbf667406ce.  Total clients: 1\n",
      "2024-03-25 13:19:36,214 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:be1914b2-515f-42e2-b7bd-7cbf667406ce SSID:\n",
      "2024-03-25 13:19:36,218 - ClientManager - INFO - Client: New client site-2@100.122.101.102 joined. Sent token: 7cea3706-6751-4161-9973-873766a8dc19.  Total clients: 2\n",
      "2024-03-25 13:19:36,221 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:7cea3706-6751-4161-9973-873766a8dc19 SSID:\n",
      "2024-03-25 13:19:36,222 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2024-03-25 13:19:36,224 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2024-03-25 13:19:36,228 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2024-03-25 13:19:36,230 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-03-25 13:19:36,232 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2024-03-25 13:19:36,771 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2024-03-25 13:19:36,773 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow DhPSIController (<class 'nvflare.app_common.psi.dh_psi.dh_psi_controller.DhPSIController'>) ...\n",
      "2024-03-25 13:19:36,774 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Workflow DhPSIController (<class 'nvflare.app_common.psi.dh_psi.dh_psi_controller.DhPSIController'>) started\n",
      "2024-03-25 13:19:36,776 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: PSI control flow started.\n",
      "2024-03-25 13:19:36,778 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: start pre workflow\n",
      "2024-03-25 13:19:36,779 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: pre_process on task PSI\n",
      "2024-03-25 13:19:36,781 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:19:37,228 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2024-03-25 13:19:38,231 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2024-03-25 13:19:38,234 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2024-03-25 13:19:39,324 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00005 127.0.0.1:47335 <= 127.0.0.1:52960] is created: PID: 4011917\n",
      "2024-03-25 13:19:39,341 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:47335 <= 127.0.0.1:52962] is created: PID: 4011917\n",
      "2024-03-25 13:19:39,280 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2024-03-25 13:19:39,295 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2024-03-25 13:19:39,321 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:47335\n",
      "2024-03-25 13:19:39,321 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:47335] is starting\n",
      "2024-03-25 13:19:39,323 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:52960 => 127.0.0.1:47335] is created: PID: 4012018\n",
      "2024-03-25 13:19:39,340 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:47335\n",
      "2024-03-25 13:19:39,340 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:47335] is starting\n",
      "2024-03-25 13:19:39,341 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:52962 => 127.0.0.1:47335] is created: PID: 4012019\n",
      "2024-03-25 13:19:41,058 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-03-25 13:19:41,065 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-03-25 13:19:41,580 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: assigned task to client site-2: name=PSI, id=55510391-2459-4090-a018-92805628b746\n",
      "2024-03-25 13:19:41,583 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: sent task assignment to client. client_name:site-2 task_id:55510391-2459-4090-a018-92805628b746\n",
      "2024-03-25 13:19:41,587 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: assigned task to client site-1: name=PSI, id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "2024-03-25 13:19:41,588 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: PSI   task_id: 55510391-2459-4090-a018-92805628b746  sharable_header_task_id: 55510391-2459-4090-a018-92805628b746\n",
      "2024-03-25 13:19:41,591 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: sent task assignment to client. client_name:site-1 task_id:22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "2024-03-25 13:19:41,595 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: PSI   task_id: 22ee9d0a-cd66-4bea-8e7d-4c6de009e77d  sharable_header_task_id: 22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "2024-03-25 13:19:41,662 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=PSI, id=55510391-2459-4090-a018-92805628b746\n",
      "2024-03-25 13:19:41,665 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=PSI, id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "task_name PSI\n",
      "2024-03-25 13:19:41,674 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: Processing PSI, None result from client site-2\n",
      "2024-03-25 13:19:41,676 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: Received result from client:site-2 for task PSI \n",
      "2024-03-25 13:19:41,678 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:19:41,680 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:55510391-2459-4090-a018-92805628b746\n",
      "task_name PSI\n",
      "2024-03-25 13:19:41,689 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: Processing PSI, None result from client site-1\n",
      "2024-03-25 13:19:41,691 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: Received result from client:site-1 for task PSI \n",
      "2024-03-25 13:19:41,693 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:19:41,695 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "2024-03-25 13:19:41,563 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2024-03-25 13:19:41,570 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2024-03-25 13:19:41,574 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5118887424468994 seconds\n",
      "2024-03-25 13:19:41,575 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2024-03-25 13:19:41,575 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2024-03-25 13:19:41,579 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5103192329406738 seconds\n",
      "2024-03-25 13:19:41,580 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2024-03-25 13:19:41,580 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2024-03-25 13:19:41,596 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 519B (519 Bytes) time: 0.019894 seconds\n",
      "2024-03-25 13:19:41,596 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:19:41,597 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=55510391-2459-4090-a018-92805628b746\n",
      "2024-03-25 13:19:41,598 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:19:41,598 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: Executing task 'PSI' for site-2\n",
      "2024-03-25 13:19:41,599 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: Executing psi_stage_task PSI_PREPARE for site-2\n",
      "2024-03-25 13:19:41,601 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 519B (519 Bytes) time: 0.019932 seconds\n",
      "2024-03-25 13:19:41,601 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:19:41,602 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d\n",
      "2024-03-25 13:19:41,603 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:19:41,603 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: Executing task 'PSI' for site-1\n",
      "2024-03-25 13:19:41,603 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: Executing psi_stage_task PSI_PREPARE for site-1\n",
      "2024-03-25 13:19:41,652 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: finished processing task\n",
      "2024-03-25 13:19:41,652 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: try #1: sending task result to server\n",
      "2024-03-25 13:19:41,653 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: checking task ...\n",
      "2024-03-25 13:19:41,653 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:19:41,654 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: finished processing task\n",
      "2024-03-25 13:19:41,655 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: try #1: sending task result to server\n",
      "2024-03-25 13:19:41,655 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: checking task ...\n",
      "2024-03-25 13:19:41,655 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:19:41,659 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: start to send task result to server\n",
      "2024-03-25 13:19:41,659 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:19:41,662 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: start to send task result to server\n",
      "2024-03-25 13:19:41,662 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:19:41,687 - Communicator - INFO -  SubmitUpdate size: 656B (656 Bytes). time: 0.027634 seconds\n",
      "2024-03-25 13:19:41,688 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=55510391-2459-4090-a018-92805628b746]: task result sent to server\n",
      "2024-03-25 13:19:41,688 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:19:41,699 - Communicator - INFO -  SubmitUpdate size: 656B (656 Bytes). time: 0.036879 seconds\n",
      "2024-03-25 13:19:41,699 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=22ee9d0a-cd66-4bea-8e7d-4c6de009e77d]: task result sent to server\n",
      "2024-03-25 13:19:41,700 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:19:41,787 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:19:41,789 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: PSI_PREPARE results = {'site-2': <nvflare.apis.dxo.DXO object at 0x7f705bc32c80>, 'site-1': <nvflare.apis.dxo.DXO object at 0x7f705bb80190>}\n",
      "2024-03-25 13:19:41,792 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: start workflow\n",
      "2024-03-25 13:19:41,794 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: order sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)]\n",
      "2024-03-25 13:19:41,795 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: target_sites: [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)]\n",
      "2024-03-25 13:19:41,797 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:19:41,799 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-2 task\n",
      "2024-03-25 13:19:43,695 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: assigned task to client site-2: name=PSI, id=0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "2024-03-25 13:19:43,698 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: sent task assignment to client. client_name:site-2 task_id:0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "2024-03-25 13:19:43,701 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: PSI   task_id: 0a973e4d-f5d8-480c-a05c-f4ea97310889  sharable_header_task_id: 0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "2024-03-25 13:19:43,705 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 505B (505 Bytes) time: 0.013521 seconds\n",
      "2024-03-25 13:19:43,706 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:19:43,706 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "2024-03-25 13:19:43,707 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:19:43,707 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: Executing task 'PSI' for site-2\n",
      "2024-03-25 13:19:43,708 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: Executing psi_stage_task PSI_SETUP for site-2\n",
      "2024-03-25 13:19:48,824 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=PSI, id=0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "task_name PSI\n",
      "2024-03-25 13:19:48,831 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: Processing PSI, None result from client site-2\n",
      "2024-03-25 13:19:48,833 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: Received result from client:site-2 for task PSI \n",
      "2024-03-25 13:19:48,835 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:19:48,836 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:0a973e4d-f5d8-480c-a05c-f4ea97310889\n",
      "2024-03-25 13:19:49,006 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:19:49,010 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:19:49,012 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-1 task\n",
      "2024-03-25 13:19:48,813 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: finished processing task\n",
      "2024-03-25 13:19:48,813 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: try #1: sending task result to server\n",
      "2024-03-25 13:19:48,813 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: checking task ...\n",
      "2024-03-25 13:19:48,813 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:19:48,818 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: start to send task result to server\n",
      "2024-03-25 13:19:48,819 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:19:48,841 - Communicator - INFO -  SubmitUpdate size: 470.9KB (470902 Bytes). time: 0.022125 seconds\n",
      "2024-03-25 13:19:48,841 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=0a973e4d-f5d8-480c-a05c-f4ea97310889]: task result sent to server\n",
      "2024-03-25 13:19:48,842 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:19:49,745 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: assigned task to client site-1: name=PSI, id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "2024-03-25 13:19:49,748 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: sent task assignment to client. client_name:site-1 task_id:1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "2024-03-25 13:19:49,750 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: PSI   task_id: 1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe  sharable_header_task_id: 1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "2024-03-25 13:19:49,759 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 470.8KB (470753 Bytes) time: 0.016674 seconds\n",
      "2024-03-25 13:19:49,759 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:19:49,759 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "2024-03-25 13:19:49,760 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:19:49,760 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: Executing task 'PSI' for site-1\n",
      "2024-03-25 13:19:49,761 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: Executing psi_stage_task PSI_REQUEST for site-1\n",
      "2024-03-25 13:19:54,815 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=PSI, id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "task_name PSI\n",
      "2024-03-25 13:19:54,831 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: Processing PSI, None result from client site-1\n",
      "2024-03-25 13:19:54,833 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: Received result from client:site-1 for task PSI \n",
      "2024-03-25 13:19:54,835 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:19:54,836 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:19:54,838 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe\n",
      "2024-03-25 13:19:54,793 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: finished processing task\n",
      "2024-03-25 13:19:54,793 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: try #1: sending task result to server\n",
      "2024-03-25 13:19:54,793 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: checking task ...\n",
      "2024-03-25 13:19:54,793 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:19:54,798 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: start to send task result to server\n",
      "2024-03-25 13:19:54,798 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:19:54,843 - Communicator - INFO -  SubmitUpdate size: 1.8MB (1750661 Bytes). time: 0.044651 seconds\n",
      "2024-03-25 13:19:54,843 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=1d1340f1-98ff-4ae3-9adb-336ca4a3dcbe]: task result sent to server\n",
      "2024-03-25 13:19:54,844 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:19:55,022 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:19:55,024 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-2 task\n",
      "2024-03-25 13:19:56,884 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: assigned task to client site-2: name=PSI, id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "2024-03-25 13:19:56,887 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: sent task assignment to client. client_name:site-2 task_id:8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "2024-03-25 13:19:56,888 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: PSI   task_id: 8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f  sharable_header_task_id: 8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "2024-03-25 13:19:56,907 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 1.8MB (1750513 Bytes) time: 0.026170 seconds\n",
      "2024-03-25 13:19:56,908 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:19:56,908 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "2024-03-25 13:19:56,909 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:19:56,909 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: Executing task 'PSI' for site-2\n",
      "2024-03-25 13:19:56,909 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: Executing psi_stage_task PSI_RESPONSE for site-2\n",
      "2024-03-25 13:20:00,029 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=PSI, id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "task_name PSI\n",
      "2024-03-25 13:20:00,037 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: Processing PSI, None result from client site-2\n",
      "2024-03-25 13:20:00,039 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: Received result from client:site-2 for task PSI \n",
      "2024-03-25 13:20:00,041 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:00,043 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f\n",
      "2024-03-25 13:20:00,051 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:00,010 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: finished processing task\n",
      "2024-03-25 13:20:00,014 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: try #1: sending task result to server\n",
      "2024-03-25 13:20:00,014 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: checking task ...\n",
      "2024-03-25 13:20:00,014 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:00,019 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: start to send task result to server\n",
      "2024-03-25 13:20:00,019 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:00,047 - Communicator - INFO -  SubmitUpdate size: 1.8MB (1750660 Bytes). time: 0.026971 seconds\n",
      "2024-03-25 13:20:00,047 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=8ec2fd67-4f88-4279-b1f3-a9e2eee4d97f]: task result sent to server\n",
      "2024-03-25 13:20:00,047 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:00,233 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:20:00,235 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-1 task\n",
      "2024-03-25 13:20:00,376 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: assigned task to client site-1: name=PSI, id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "2024-03-25 13:20:00,379 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: sent task assignment to client. client_name:site-1 task_id:ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "2024-03-25 13:20:00,381 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: PSI   task_id: ca60ae2b-0114-433e-97c7-cd18db3fa5a0  sharable_header_task_id: ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "2024-03-25 13:20:00,398 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 1.8MB (1750518 Bytes) time: 0.025069 seconds\n",
      "2024-03-25 13:20:00,398 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:20:00,398 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "2024-03-25 13:20:00,399 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:20:00,399 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: Executing task 'PSI' for site-1\n",
      "2024-03-25 13:20:00,400 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: Executing psi_stage_task PSI_TASK_INTERSECT for site-1\n",
      "2024-03-25 13:20:03,581 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=PSI, id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "task_name PSI\n",
      "2024-03-25 13:20:03,588 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: Processing PSI, None result from client site-1\n",
      "2024-03-25 13:20:03,589 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: Received result from client:site-1 for task PSI \n",
      "2024-03-25 13:20:03,591 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:03,593 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:ca60ae2b-0114-433e-97c7-cd18db3fa5a0\n",
      "2024-03-25 13:20:03,660 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:03,573 - FilePSIWriter - INFO - [identity=site-1, run=simulate_job]: job dir = /tmp/nvflare/cifar10_psi/simulate_job\n",
      "2024-03-25 13:20:03,573 - FilePSIWriter - INFO - [identity=site-1, run=simulate_job]: trying to save data to /tmp/nvflare/cifar10_psi/simulate_job/site-1/psi/intersection.txt\n",
      "2024-03-25 13:20:03,574 - FilePSIWriter - INFO - [identity=site-1, run=simulate_job]: file /tmp/nvflare/cifar10_psi/simulate_job/site-1/psi/intersection.txt saved\n",
      "2024-03-25 13:20:03,574 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: finished processing task\n",
      "2024-03-25 13:20:03,574 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: try #1: sending task result to server\n",
      "2024-03-25 13:20:03,574 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: checking task ...\n",
      "2024-03-25 13:20:03,574 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:03,579 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: start to send task result to server\n",
      "2024-03-25 13:20:03,579 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:03,597 - Communicator - INFO -  SubmitUpdate size: 656B (656 Bytes). time: 0.017466 seconds\n",
      "2024-03-25 13:20:03,597 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=ca60ae2b-0114-433e-97c7-cd18db3fa5a0]: task result sent to server\n",
      "2024-03-25 13:20:03,597 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:03,842 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: target_sites: [SiteSize(name='site-1', size=10001)]\n",
      "2024-03-25 13:20:03,845 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: forward_processed sites {'site-1': 10001}\n",
      ",intersect_sites=SiteSize(name='site-1', size=10001)\n",
      "ordered sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)]\n",
      "\n",
      "2024-03-25 13:20:03,847 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:20:05,602 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: assigned task to client site-1: name=PSI, id=05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "2024-03-25 13:20:05,605 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: sent task assignment to client. client_name:site-1 task_id:05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "2024-03-25 13:20:05,607 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: PSI   task_id: 05211e89-44af-4679-a6b5-371c5cde35ec  sharable_header_task_id: 05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "2024-03-25 13:20:05,612 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 543B (543 Bytes) time: 0.013139 seconds\n",
      "2024-03-25 13:20:05,613 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:20:05,613 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "2024-03-25 13:20:05,614 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:20:05,614 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: Executing task 'PSI' for site-1\n",
      "2024-03-25 13:20:05,614 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: Executing psi_stage_task PSI_SETUP for site-1\n",
      "2024-03-25 13:20:06,854 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=PSI, id=05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "task_name PSI\n",
      "2024-03-25 13:20:06,862 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: Processing PSI, None result from client site-1\n",
      "2024-03-25 13:20:06,863 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: Received result from client:site-1 for task PSI \n",
      "2024-03-25 13:20:06,865 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:06,867 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:05211e89-44af-4679-a6b5-371c5cde35ec\n",
      "2024-03-25 13:20:06,869 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:06,845 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: finished processing task\n",
      "2024-03-25 13:20:06,845 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: try #1: sending task result to server\n",
      "2024-03-25 13:20:06,845 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: checking task ...\n",
      "2024-03-25 13:20:06,845 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:06,850 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: start to send task result to server\n",
      "2024-03-25 13:20:06,850 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:06,872 - Communicator - INFO -  SubmitUpdate size: 470.9KB (470909 Bytes). time: 0.021512 seconds\n",
      "2024-03-25 13:20:06,872 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=05211e89-44af-4679-a6b5-371c5cde35ec]: task result sent to server\n",
      "2024-03-25 13:20:06,873 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:07,054 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:20:07,056 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-2 task\n",
      "2024-03-25 13:20:08,094 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: assigned task to client site-2: name=PSI, id=d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "2024-03-25 13:20:08,096 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: sent task assignment to client. client_name:site-2 task_id:d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "2024-03-25 13:20:08,099 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: PSI   task_id: d97864ce-5b23-4a40-a9a8-73d9fb532b26  sharable_header_task_id: d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "2024-03-25 13:20:08,105 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 470.8KB (470753 Bytes) time: 0.015307 seconds\n",
      "2024-03-25 13:20:08,106 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:20:08,106 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "2024-03-25 13:20:08,107 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:20:08,107 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: Executing task 'PSI' for site-2\n",
      "2024-03-25 13:20:08,107 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: Executing psi_stage_task PSI_REQUEST for site-2\n",
      "2024-03-25 13:20:13,216 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=PSI, id=d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "task_name PSI\n",
      "2024-03-25 13:20:13,225 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: Processing PSI, None result from client site-2\n",
      "2024-03-25 13:20:13,226 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: Received result from client:site-2 for task PSI \n",
      "2024-03-25 13:20:13,228 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:13,229 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:d97864ce-5b23-4a40-a9a8-73d9fb532b26\n",
      "2024-03-25 13:20:13,285 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:13,203 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: finished processing task\n",
      "2024-03-25 13:20:13,203 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: try #1: sending task result to server\n",
      "2024-03-25 13:20:13,203 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: checking task ...\n",
      "2024-03-25 13:20:13,203 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:13,207 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: start to send task result to server\n",
      "2024-03-25 13:20:13,207 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:13,233 - Communicator - INFO -  SubmitUpdate size: 1.8MB (1750661 Bytes). time: 0.025533 seconds\n",
      "2024-03-25 13:20:13,233 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=d97864ce-5b23-4a40-a9a8-73d9fb532b26]: task result sent to server\n",
      "2024-03-25 13:20:13,234 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:13,467 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:20:14,915 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: assigned task to client site-1: name=PSI, id=4a215283-39fb-431f-8d0e-278055087c7a\n",
      "2024-03-25 13:20:14,918 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: sent task assignment to client. client_name:site-1 task_id:4a215283-39fb-431f-8d0e-278055087c7a\n",
      "2024-03-25 13:20:14,920 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: PSI   task_id: 4a215283-39fb-431f-8d0e-278055087c7a  sharable_header_task_id: 4a215283-39fb-431f-8d0e-278055087c7a\n",
      "2024-03-25 13:20:14,934 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 1.8MB (1750525 Bytes) time: 0.022727 seconds\n",
      "2024-03-25 13:20:14,935 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:20:14,935 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=4a215283-39fb-431f-8d0e-278055087c7a\n",
      "2024-03-25 13:20:14,936 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:20:14,936 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: Executing task 'PSI' for site-1\n",
      "2024-03-25 13:20:14,936 - DhPSITaskHandler - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: Executing psi_stage_task PSI_RESPONSE for site-1\n",
      "2024-03-25 13:20:18,100 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=PSI, id=4a215283-39fb-431f-8d0e-278055087c7a\n",
      "task_name PSI\n",
      "2024-03-25 13:20:18,107 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: Processing PSI, None result from client site-1\n",
      "2024-03-25 13:20:18,109 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: Received result from client:site-1 for task PSI \n",
      "2024-03-25 13:20:18,111 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:18,114 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:4a215283-39fb-431f-8d0e-278055087c7a\n",
      "2024-03-25 13:20:18,078 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: finished processing task\n",
      "2024-03-25 13:20:18,078 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: try #1: sending task result to server\n",
      "2024-03-25 13:20:18,086 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: checking task ...\n",
      "2024-03-25 13:20:18,087 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:18,092 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: start to send task result to server\n",
      "2024-03-25 13:20:18,092 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:18,117 - Communicator - INFO -  SubmitUpdate size: 1.8MB (1750668 Bytes). time: 0.025146 seconds\n",
      "2024-03-25 13:20:18,117 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=4a215283-39fb-431f-8d0e-278055087c7a]: task result sent to server\n",
      "2024-03-25 13:20:18,118 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:18,298 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:18,476 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: scheduled task PSI\n",
      "2024-03-25 13:20:18,478 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: wait for client site-2 task\n",
      "2024-03-25 13:20:18,764 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: assigned task to client site-2: name=PSI, id=e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "2024-03-25 13:20:18,767 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: sent task assignment to client. client_name:site-2 task_id:e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "2024-03-25 13:20:18,770 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: PSI   task_id: e9653c3f-ef90-4221-aa13-066fce3b5bc0  sharable_header_task_id: e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "2024-03-25 13:20:18,781 - Communicator - INFO - Received from simulator_server server. getTask: PSI size: 1.8MB (1750518 Bytes) time: 0.019885 seconds\n",
      "2024-03-25 13:20:18,781 - FederatedClient - INFO - pull_task completed. Task name:PSI Status:True \n",
      "2024-03-25 13:20:18,781 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=PSI, id=e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "2024-03-25 13:20:18,782 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: invoking task executor PSIExecutor\n",
      "2024-03-25 13:20:18,782 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: Executing task 'PSI' for site-2\n",
      "2024-03-25 13:20:18,783 - DhPSITaskHandler - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: Executing psi_stage_task PSI_TASK_INTERSECT for site-2\n",
      "2024-03-25 13:20:21,939 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=PSI, id=e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "task_name PSI\n",
      "2024-03-25 13:20:21,947 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: Processing PSI, None result from client site-2\n",
      "2024-03-25 13:20:21,949 - BroadcastAndWait - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: Received result from client:site-2 for task PSI \n",
      "2024-03-25 13:20:21,950 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: finished processing client result by DhPSIController\n",
      "2024-03-25 13:20:21,952 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:e9653c3f-ef90-4221-aa13-066fce3b5bc0\n",
      "2024-03-25 13:20:22,108 - DhPSIController - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: task PSI exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:21,930 - FilePSIWriter - INFO - [identity=site-2, run=simulate_job]: job dir = /tmp/nvflare/cifar10_psi/simulate_job\n",
      "2024-03-25 13:20:21,930 - FilePSIWriter - INFO - [identity=site-2, run=simulate_job]: trying to save data to /tmp/nvflare/cifar10_psi/simulate_job/site-2/psi/intersection.txt\n",
      "2024-03-25 13:20:21,931 - FilePSIWriter - INFO - [identity=site-2, run=simulate_job]: file /tmp/nvflare/cifar10_psi/simulate_job/site-2/psi/intersection.txt saved\n",
      "2024-03-25 13:20:21,931 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: finished processing task\n",
      "2024-03-25 13:20:21,931 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: try #1: sending task result to server\n",
      "2024-03-25 13:20:21,931 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: checking task ...\n",
      "2024-03-25 13:20:21,931 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:21,936 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: start to send task result to server\n",
      "2024-03-25 13:20:21,936 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:21,956 - Communicator - INFO -  SubmitUpdate size: 656B (656 Bytes). time: 0.019025 seconds\n",
      "2024-03-25 13:20:21,956 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=PSI, task_id=e9653c3f-ef90-4221-aa13-066fce3b5bc0]: task result sent to server\n",
      "2024-03-25 13:20:21,956 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:22,285 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: received intersections : {'site-2': 10000} \n",
      "2024-03-25 13:20:22,288 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: parallel_back_pass took 18441.199944354594 (ms)\n",
      "2024-03-25 13:20:22,290 - DhPSIWorkFlow - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: backward_processed sites {'site-2': 10000}\n",
      ",intersect_sites=SiteSize(name='site-1', size=10001)\n",
      "ordered sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)]\n",
      "\n",
      "2024-03-25 13:20:22,292 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Exception in workflow DhPSIController: RuntimeError: Intersection calculation failed: the intersection sizes from all sites must be equal.\n",
      "backward processed sites:{'site-2': 10000},\n",
      "intersect sites =SiteSize(name='site-1', size=10001) \n",
      "ordered sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)] \n",
      "\n",
      "2024-03-25 13:20:22,296 - ServerRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/fed/server/server_runner.py\", line 140, in _execute_run\n",
      "    wf.responder.control_flow(self.abort_signal, fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/app_common/psi/psi_controller.py\", line 47, in control_flow\n",
      "    self.psi_workflow.run(abort_signal)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/app_common/psi/dh_psi/dh_psi_workflow.py\", line 86, in run\n",
      "    self.check_final_intersection_sizes(intersect_site)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/app_common/psi/dh_psi/dh_psi_workflow.py\", line 103, in check_final_intersection_sizes\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Intersection calculation failed: the intersection sizes from all sites must be equal.\n",
      "backward processed sites:{'site-2': 10000},\n",
      "intersect sites =SiteSize(name='site-1', size=10001) \n",
      "ordered sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)] \n",
      "\n",
      "\n",
      "2024-03-25 13:20:22,298 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Aborting current RUN due to FATAL_SYSTEM_ERROR received: Exception in workflow DhPSIController: RuntimeError: Intersection calculation failed: the intersection sizes from all sites must be equal.\n",
      "backward processed sites:{'site-2': 10000},\n",
      "intersect sites =SiteSize(name='site-1', size=10001) \n",
      "ordered sites = [SiteSize(name='site-2', size=50000), SiteSize(name='site-1', size=50000)] \n",
      "\n",
      "2024-03-25 13:20:22,300 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2024-03-25 13:20:22,302 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Workflow: DhPSIController finalizing ...\n",
      "2024-03-25 13:20:22,311 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:20:22,313 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:20:22,315 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: END_RUN fired\n",
      "2024-03-25 13:20:22,317 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: Server runner finished.\n",
      "2024-03-25 13:20:23,142 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-1, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2024-03-25 13:20:23,144 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2024-03-25 13:20:23,197 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00005 Not Connected] is closed PID: 4011917\n",
      "2024-03-25 13:20:23,149 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2024-03-25 13:20:23,150 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2024-03-25 13:20:23,150 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: started end-run events sequence\n",
      "2024-03-25 13:20:23,150 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:20:23,151 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:20:23,151 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: END_RUN fired\n",
      "2024-03-25 13:20:23,151 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2024-03-25 13:20:23,194 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-1 \n",
      "2024-03-25 13:20:23,197 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 4012018\n",
      "2024-03-25 13:20:23,961 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController, peer=site-2, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2024-03-25 13:20:23,964 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2024-03-25 13:20:24,018 - FederatedClient - INFO - Shutting down client run: site-1\n",
      "2024-03-25 13:20:24,020 - FederatedClient - INFO - Shutting down client run: site-2\n",
      "2024-03-25 13:20:24,021 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=DhPSIController]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2024-03-25 13:20:24,022 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 Not Connected] is closed PID: 4011917\n",
      "2024-03-25 13:20:23,969 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2024-03-25 13:20:23,970 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2024-03-25 13:20:23,970 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: started end-run events sequence\n",
      "2024-03-25 13:20:23,970 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:20:23,970 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:20:23,971 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: END_RUN fired\n",
      "2024-03-25 13:20:23,971 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2024-03-25 13:20:24,018 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-2 \n",
      "2024-03-25 13:20:24,022 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 4012019\n",
      "2024-03-25 13:20:24,806 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2024-03-25 13:20:25,248 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 60799 shutdown!\n",
      "2024-03-25 13:20:25,251 - SimulatorServer - INFO - shutting down server\n",
      "2024-03-25 13:20:25,253 - SimulatorServer - INFO - canceling sync locks\n",
      "2024-03-25 13:20:25,254 - SimulatorServer - INFO - server off\n",
      "2024-03-25 13:20:28,699 - MPM - INFO - MPM: Good Bye!\n",
      "INFO:SimulatorRunner:return_code from process.exitcode: 0\n",
      "Simulator finished with run_status 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nvflare import SimulatorRunner\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=f\"jobs/cifar10_psi\",\n",
    "    workspace=\"/tmp/nvflare/cifar10_psi\",\n",
    "    n_clients=2,\n",
    "    threads=2\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1388dc-6a4f-4965-a09f-4d058fc3833c",
   "metadata": {},
   "source": [
    "The result will be saved on each client's working directory in `intersection.txt`.\n",
    "\n",
    "We can check the correctness of the result by comparing to the generate ground truth overlap, saved in `overlap.npy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb6bcc-9443-4331-bde3-4576fbfffaec",
   "metadata": {},
   "source": [
    "### Check the PSI result\n",
    "We can check the correctness of the result by comparing to the generate ground truth overlap, saved in overlap.npy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a6b36f-649f-4e19-ba0a-5dd71dfda5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_overlap [11841 19602 45519 ... 47278 37020  2217] n=10000\n",
      "psi_overlap_1 [ 4481. 45431. 46253. ... 34846.   179.  7277.] n=10001\n",
      "psi_overlap_2 [38639. 10733. 31911. ... 12172. 46167.   865.] n=10000\n",
      "Found 100.0% of the overlapping sample ids for site-1.\n",
      "Found 100.0% of the overlapping sample ids for site-2.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "gt_overlap = np.load(os.path.join(SPLIT_DIR, \"overlap.npy\"))\n",
    "\n",
    "psi_overlap_1 = np.loadtxt(\"/tmp/nvflare/cifar10_psi/simulate_job/site-1/psi/intersection.txt\")\n",
    "psi_overlap_2 = np.loadtxt(\"/tmp/nvflare/cifar10_psi/simulate_job/site-2/psi/intersection.txt\")\n",
    "                     \n",
    "print(\"gt_overlap\", gt_overlap, f\"n={len(gt_overlap)}\")\n",
    "print(\"psi_overlap_1\", psi_overlap_1, f\"n={len(psi_overlap_1)}\")\n",
    "print(\"psi_overlap_2\", psi_overlap_2, f\"n={len(psi_overlap_2)}\")\n",
    "\n",
    "intersect_1 = np.intersect1d(psi_overlap_1, gt_overlap, assume_unique=True)\n",
    "intersect_2 = np.intersect1d(psi_overlap_2, gt_overlap, assume_unique=True)\n",
    "\n",
    "print(f\"Found {100*len(intersect_1)/len(gt_overlap):.1f}% of the overlapping sample ids for site-1.\")\n",
    "print(f\"Found {100*len(intersect_2)/len(gt_overlap):.1f}% of the overlapping sample ids for site-2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0713e2-e393-41c0-9da0-392535cf8a54",
   "metadata": {},
   "source": [
    "## 3. Run simulated split-learning experiments\n",
    "Next we use the `intersection.txt` files to align the datasets on each participating site in order to do split learning.\n",
    "The [config_fed_client.json](./jobs/cifar10_splitnn/site-1/config/config_fed_client.json) takes as input the previously generated intersection file for each site.\n",
    "```\n",
    "    {\n",
    "        \"id\": \"cifar10-learner\",\n",
    "        \"path\": \"pt.learners.cifar10_learner_splitnn.CIFAR10LearnerSplitNN\",\n",
    "        \"args\": {\n",
    "            \"dataset_root\": \"{DATASET_ROOT}\",\n",
    "            \"intersection_file\": \"{INTERSECTION_FILE}\",\n",
    "            \"lr\": 1e-2,\n",
    "            \"model\": {\"path\": \"pt.networks.split_nn.SplitNN\", \"args\":  {\"split_id\":  0}},\n",
    "            \"timeit\": true\n",
    "        }\n",
    "    }\n",
    "```\n",
    "To set the filename automatically, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3978f6ac-f7db-4648-abb3-0fd071f01531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified jobs/cifar10_splitnn/site-1/config/config_fed_client.json to use INTERSECTION_FILE=/tmp/nvflare/cifar10_psi/simulate_job/site-1/psi/intersection.txt\n",
      "Modified jobs/cifar10_splitnn/site-2/config/config_fed_client.json to use INTERSECTION_FILE=/tmp/nvflare/cifar10_psi/simulate_job/site-2/psi/intersection.txt\n"
     ]
    }
   ],
   "source": [
    "!for i in {1..2}; \\\n",
    "do \\\n",
    "  CONFIG_FILE=jobs/cifar10_splitnn/site-${i}/config/config_fed_client.json; \\\n",
    "  INTERSECTION_FILE=/tmp/nvflare/cifar10_psi/simulate_job/site-${i}/psi/intersection.txt; \\\n",
    "  python3 ./set_intersection_file.py --config_file ${CONFIG_FILE} --intersection_file ${INTERSECTION_FILE}; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f606a9-55a9-4984-a40c-7951287a5a63",
   "metadata": {},
   "source": [
    "To run the experiment, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c75dcb-014d-40c4-8a4a-7a53847c486b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:MPM:=========== MPM: started to run forever\n",
      "2024-03-25 13:20:29,182 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2024-03-25 13:20:29,188 - CoreCell - INFO - server: creating listener on tcp://0:50881\n",
      "2024-03-25 13:20:29,210 - CoreCell - INFO - server: created backbone external listener for tcp://0:50881\n",
      "2024-03-25 13:20:29,212 - ConnectorManager - INFO - 4012948: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2024-03-25 13:20:29,215 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:19366] is starting\n",
      "2024-03-25 13:20:29,717 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:19366\n",
      "2024-03-25 13:20:29,720 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:50881] is starting\n",
      "2024-03-25 13:20:29,847 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 58939\n",
      "2024-03-25 13:20:29,848 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2024-03-25 13:20:29,852 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2024-03-25 13:20:29,857 - ClientManager - INFO - Client: New client site-1@100.122.101.102 joined. Sent token: 2fd56fb0-860e-48cd-8b7e-c3f17a3fad2b.  Total clients: 1\n",
      "2024-03-25 13:20:29,859 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:2fd56fb0-860e-48cd-8b7e-c3f17a3fad2b SSID:\n",
      "2024-03-25 13:20:29,862 - ClientManager - INFO - Client: New client site-2@100.122.101.102 joined. Sent token: 78ef282a-0141-4e49-b0d3-38ac6db39056.  Total clients: 2\n",
      "2024-03-25 13:20:29,863 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:78ef282a-0141-4e49-b0d3-38ac6db39056 SSID:\n",
      "2024-03-25 13:20:29,864 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2024-03-25 13:20:29,866 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2024-03-25 13:20:29,869 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2024-03-25 13:20:29,871 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-03-25 13:20:29,872 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "2024-03-25 13:20:30,629 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2024-03-25 13:20:30,631 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow splitnn_ctl (<class 'nvflare.app_common.workflows.splitnn_workflow.SplitNNController'>) ...\n",
      "2024-03-25 13:20:30,632 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.\n",
      "2024-03-25 13:20:30,642 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: Workflow splitnn_ctl (<class 'nvflare.app_common.workflows.splitnn_workflow.SplitNNController'>) started\n",
      "2024-03-25 13:20:30,644 - SplitNNController - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: scheduled task _splitnn_task_init_model_\n",
      "2024-03-25 13:20:30,868 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2024-03-25 13:20:31,872 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2024-03-25 13:20:31,875 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2024-03-25 13:20:32,969 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00005 127.0.0.1:50881 <= 127.0.0.1:57696] is created: PID: 4012948\n",
      "2024-03-25 13:20:32,990 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:50881 <= 127.0.0.1:57698] is created: PID: 4012948\n",
      "2024-03-25 13:20:32,927 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2024-03-25 13:20:32,946 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2024-03-25 13:20:32,967 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:50881\n",
      "2024-03-25 13:20:32,967 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:50881] is starting\n",
      "2024-03-25 13:20:32,968 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:57696 => 127.0.0.1:50881] is created: PID: 4013071\n",
      "2024-03-25 13:20:32,988 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:50881\n",
      "2024-03-25 13:20:32,989 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:50881] is starting\n",
      "2024-03-25 13:20:32,990 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:57698 => 127.0.0.1:50881] is created: PID: 4013072\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "2024-03-25 13:20:35,254 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      ")\n",
      "2024-03-25 13:20:35,276 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-03-25 13:20:35,759 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2024-03-25 13:20:35,773 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.514371395111084 seconds\n",
      "2024-03-25 13:20:35,778 - CIFAR10LearnerSplitNN - INFO - [identity=site-1, run=simulate_job]: Running model SplitNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  (split_forward): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2024-03-25 13:20:35,781 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2024-03-25 13:20:35,792 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5118870735168457 seconds\n",
      "2024-03-25 13:20:35,795 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job]: Running model SplitNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  (split_forward): Sequential(\n",
      "    (0): Reshape()\n",
      "    (1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (2): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "    (4): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "2024-03-25 13:20:37,576 - CIFAR10LearnerSplitNN - INFO - [identity=site-1, run=simulate_job]: Running `split_id` 0 on site `site-1`\n",
      "2024-03-25 13:20:37,598 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job]: Running `split_id` 1 on site `site-2`\n",
      "2024-03-25 13:20:38,273 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: assigned task to client site-1: name=_splitnn_task_init_model_, id=ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:38,275 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: sent task assignment to client. client_name:site-1 task_id:ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:38,277 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: _splitnn_task_init_model_   task_id: ea1e90b2-90fd-42fd-becd-5120f96e00f7  sharable_header_task_id: ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:38,361 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: assigned task to client site-2: name=_splitnn_task_init_model_, id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:38,270 - CIFAR10LearnerSplitNN - INFO - [identity=site-1, run=simulate_job]: Training with 10001 overlapping indices of 50000.\n",
      "2024-03-25 13:20:38,270 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2024-03-25 13:20:38,270 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2024-03-25 13:20:38,293 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job]: Training with 10000 overlapping indices of 50000.\n",
      "2024-03-25 13:20:38,294 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2024-03-25 13:20:38,294 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2024-03-25 13:20:38,389 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: sent task assignment to client. client_name:site-2 task_id:9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:38,505 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: _splitnn_task_init_model_   task_id: 9d8dda7e-2072-4b98-b422-b3a0de5c09ce  sharable_header_task_id: 9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:39,123 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=_splitnn_task_init_model_, id=ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:39,125 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: finished processing client result by splitnn_ctl\n",
      "2024-03-25 13:20:39,127 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:39,210 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=_splitnn_task_init_model_, id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:39,213 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: finished processing client result by splitnn_ctl\n",
      "2024-03-25 13:20:39,215 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:39,084 - Communicator - INFO - Received from simulator_server server. getTask: _splitnn_task_init_model_ size: 94.4MB (94360379 Bytes) time: 0.813675 seconds\n",
      "2024-03-25 13:20:39,084 - FederatedClient - INFO - pull_task completed. Task name:_splitnn_task_init_model_ Status:True \n",
      "2024-03-25 13:20:39,084 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=_splitnn_task_init_model_, id=ea1e90b2-90fd-42fd-becd-5120f96e00f7\n",
      "2024-03-25 13:20:39,085 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: invoking task executor SplitNNLearnerExecutor\n",
      "2024-03-25 13:20:39,085 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: Client trainer got task: _splitnn_task_init_model_\n",
      "2024-03-25 13:20:39,085 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: Executing task _splitnn_task_init_model_...\n",
      "2024-03-25 13:20:39,085 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: Initializing model...\n",
      "2024-03-25 13:20:39,112 - CIFAR10LearnerSplitNN - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: init_model finished.\n",
      "2024-03-25 13:20:39,112 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: finished processing task\n",
      "2024-03-25 13:20:39,113 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: try #1: sending task result to server\n",
      "2024-03-25 13:20:39,113 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: checking task ...\n",
      "2024-03-25 13:20:39,113 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:39,120 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: start to send task result to server\n",
      "2024-03-25 13:20:39,120 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:39,131 - Communicator - INFO -  SubmitUpdate size: 543B (543 Bytes). time: 0.010517 seconds\n",
      "2024-03-25 13:20:39,131 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=ea1e90b2-90fd-42fd-becd-5120f96e00f7]: task result sent to server\n",
      "2024-03-25 13:20:39,131 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:39,176 - Communicator - INFO - Received from simulator_server server. getTask: _splitnn_task_init_model_ size: 94.4MB (94360379 Bytes) time: 0.881686 seconds\n",
      "2024-03-25 13:20:39,176 - FederatedClient - INFO - pull_task completed. Task name:_splitnn_task_init_model_ Status:True \n",
      "2024-03-25 13:20:39,176 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=_splitnn_task_init_model_, id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce\n",
      "2024-03-25 13:20:39,177 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: invoking task executor SplitNNLearnerExecutor\n",
      "2024-03-25 13:20:39,177 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: Client trainer got task: _splitnn_task_init_model_\n",
      "2024-03-25 13:20:39,177 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: Executing task _splitnn_task_init_model_...\n",
      "2024-03-25 13:20:39,177 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: Initializing model...\n",
      "2024-03-25 13:20:39,202 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: init_model finished.\n",
      "2024-03-25 13:20:39,202 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: finished processing task\n",
      "2024-03-25 13:20:39,202 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: try #1: sending task result to server\n",
      "2024-03-25 13:20:39,202 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: checking task ...\n",
      "2024-03-25 13:20:39,203 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:39,208 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: start to send task result to server\n",
      "2024-03-25 13:20:39,208 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:39,218 - Communicator - INFO -  SubmitUpdate size: 543B (543 Bytes). time: 0.009755 seconds\n",
      "2024-03-25 13:20:39,218 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_init_model_, task_id=9d8dda7e-2072-4b98-b422-b3a0de5c09ce]: task result sent to server\n",
      "2024-03-25 13:20:39,219 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "2024-03-25 13:20:39,388 - SplitNNController - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: task _splitnn_task_init_model_ exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:20:39,587 - SplitNNController - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: scheduled task _splitnn_task_train_\n",
      "2024-03-25 13:20:41,138 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: assigned task to client site-1: name=_splitnn_task_train_, id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "2024-03-25 13:20:41,141 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: sent task assignment to client. client_name:site-1 task_id:bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "2024-03-25 13:20:41,143 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: _splitnn_task_train_   task_id: bcbde2b2-b4e5-41c4-85b5-cfb8545363d5  sharable_header_task_id: bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "2024-03-25 13:20:41,225 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: assigned task to client site-2: name=_splitnn_task_train_, id=e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,228 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: sent task assignment to client. client_name:site-2 task_id:e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,230 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: _splitnn_task_train_   task_id: e861af73-071f-4c2e-bdf4-b155e68cab7f  sharable_header_task_id: e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,248 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=_splitnn_task_train_, id=e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,251 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: finished processing client result by splitnn_ctl\n",
      "2024-03-25 13:20:41,253 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,149 - Communicator - INFO - Received from simulator_server server. getTask: _splitnn_task_train_ size: 568B (568 Bytes) time: 0.014883 seconds\n",
      "2024-03-25 13:20:41,150 - FederatedClient - INFO - pull_task completed. Task name:_splitnn_task_train_ Status:True \n",
      "2024-03-25 13:20:41,150 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=_splitnn_task_train_, id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "2024-03-25 13:20:41,151 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: invoking task executor SplitNNLearnerExecutor\n",
      "2024-03-25 13:20:41,151 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: Client trainer got task: _splitnn_task_train_\n",
      "2024-03-25 13:20:41,151 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: Executing task _splitnn_task_train_...\n",
      "2024-03-25 13:20:41,151 - SplitNNLearnerExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: Running training...\n",
      "2024-03-25 13:20:41,152 - CIFAR10LearnerSplitNN - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: Starting training of 4000 rounds with batch size 32\n",
      "2024-03-25 13:20:41,234 - Communicator - INFO - Received from simulator_server server. getTask: _splitnn_task_train_ size: 568B (568 Bytes) time: 0.012591 seconds\n",
      "2024-03-25 13:20:41,235 - FederatedClient - INFO - pull_task completed. Task name:_splitnn_task_train_ Status:True \n",
      "2024-03-25 13:20:41,235 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=_splitnn_task_train_, id=e861af73-071f-4c2e-bdf4-b155e68cab7f\n",
      "2024-03-25 13:20:41,236 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: invoking task executor SplitNNLearnerExecutor\n",
      "2024-03-25 13:20:41,236 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: Client trainer got task: _splitnn_task_train_\n",
      "2024-03-25 13:20:41,236 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: Executing task _splitnn_task_train_...\n",
      "2024-03-25 13:20:41,236 - SplitNNLearnerExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: Running training...\n",
      "2024-03-25 13:20:41,236 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: Starting training of 4000 rounds with batch size 32\n",
      "2024-03-25 13:20:41,238 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: finished processing task\n",
      "2024-03-25 13:20:41,238 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: try #1: sending task result to server\n",
      "2024-03-25 13:20:41,239 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: checking task ...\n",
      "2024-03-25 13:20:41,239 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:20:41,245 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: start to send task result to server\n",
      "2024-03-25 13:20:41,245 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:20:41,256 - Communicator - INFO -  SubmitUpdate size: 538B (538 Bytes). time: 0.011026 seconds\n",
      "2024-03-25 13:20:41,257 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=e861af73-071f-4c2e-bdf4-b155e68cab7f]: task result sent to server\n",
      "2024-03-25 13:20:41,257 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 2 task_processed: True\n",
      "0\n",
      "2024-03-25 13:20:42,925 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_train_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:20:44,990 - CIFAR10LearnerSplitNN - INFO - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: Round 0/4000 train_loss: 2.3842, train_accuracy: 0.0938\n",
      "0\n",
      "2024-03-25 13:20:45,080 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,195 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,307 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,431 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,543 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,632 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,756 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,861 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:45,937 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,027 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,134 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,212 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,300 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,410 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,488 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,607 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,728 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,807 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,896 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:46,974 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,092 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,184 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,284 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,374 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,460 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,560 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,636 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,716 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,821 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,903 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:47,983 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,093 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,185 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,309 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,416 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,504 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,583 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,733 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,825 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:48,904 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,017 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,109 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,187 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,289 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,375 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,453 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,566 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,653 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,733 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,894 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:49,988 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,110 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,221 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,308 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,388 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,507 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,619 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,691 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,811 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,910 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:50,989 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,116 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,220 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,298 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,381 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,532 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,620 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,698 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,810 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,901 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:51,983 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,090 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,183 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,307 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,414 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,504 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,585 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,697 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,789 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,862 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:52,943 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,035 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,143 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,223 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,313 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,417 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,495 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,617 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,725 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,841 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:53,928 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,088 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,169 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,260 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,366 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,445 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,532 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,642 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,720 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,808 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:54,913 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,033 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,119 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,219 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,301 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,421 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,546 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,617 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,708 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,788 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,864 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:55,954 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,044 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,122 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,195 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,284 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,365 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,448 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,541 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,622 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,698 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,790 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,912 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:56,993 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,067 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,156 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,238 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,317 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,404 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,524 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,602 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,726 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,815 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,893 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:57,963 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,054 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,136 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,217 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,303 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,385 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,473 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,560 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,680 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,768 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,895 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:58,977 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,072 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,148 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,271 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,360 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,441 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,519 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,646 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,725 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,808 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,888 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:20:59,975 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,102 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,183 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,277 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,402 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,468 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,555 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,637 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,754 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,844 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:00,971 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,050 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,176 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,258 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,339 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,414 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,502 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,600 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,680 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,808 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,896 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:01,977 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,109 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,196 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,315 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,397 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,483 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,563 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,694 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,784 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,859 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:02,947 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,035 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,116 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,200 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,288 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,367 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,456 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,578 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,654 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,781 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,873 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:03,950 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,037 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,124 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,247 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,333 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,427 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,550 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,635 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,728 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,822 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,901 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:04,985 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,116 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,190 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,280 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,370 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,447 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,575 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,668 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,738 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,821 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:05,946 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,027 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,107 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,198 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,322 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,399 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "0\n",
      "2024-03-25 13:21:06,556 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2024-03-25 13:21:06,749 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:06,750 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.06 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:06,781 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:06,998 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:06,999 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,020 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,246 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,247 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,268 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,350 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,350 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,382 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,464 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,464 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,499 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,583 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,583 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,616 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,692 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,692 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 25.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,722 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:07,917 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:07,918 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:07,949 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,027 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,028 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,060 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,144 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,145 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,178 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,260 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,260 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,286 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,364 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,364 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,392 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,473 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,474 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,500 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,579 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,579 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,611 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,701 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,702 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,730 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,817 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,818 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,844 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:08,919 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:08,919 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 17.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.83 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 498.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:08,945 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,014 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,014 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 21.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,040 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,118 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,118 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,156 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,231 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,231 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,257 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,345 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,346 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,379 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,457 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,458 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,491 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,572 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,573 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,599 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,674 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,674 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,706 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,830 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,830 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,864 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:09,952 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:09,953 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:09,980 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,063 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,064 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,096 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,212 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,212 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,238 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,320 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,320 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,349 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,426 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,426 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,459 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,537 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,538 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,571 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,647 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,647 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,680 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,769 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,770 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,804 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:10,899 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:10,899 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:10,932 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,011 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,012 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,049 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,131 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,132 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,164 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,250 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,251 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,283 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,362 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,362 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,395 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,477 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,477 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,504 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,585 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,586 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,613 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,690 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,690 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,716 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,803 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,804 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,839 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:11,915 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:11,915 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:11,941 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,011 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,011 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,036 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,110 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,110 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 36.69 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,136 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,214 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,214 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.44 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,248 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,326 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,326 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,352 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,426 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,426 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,451 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,531 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,532 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,558 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,635 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,635 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,661 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,746 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,747 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,776 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,861 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,861 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,888 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:12,968 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:12,968 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:12,996 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,069 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,070 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,096 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,169 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,170 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,195 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,264 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,264 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,290 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,408 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,408 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,445 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,522 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,522 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,555 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,637 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,638 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,669 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,752 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,753 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,779 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,859 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,859 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,885 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:13,965 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:13,966 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:13,998 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,079 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,080 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 67, in forward\n",
      "    out = self.bn3(self.conv3(out))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 37.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.84 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 495.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,117 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,208 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,208 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.62 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,243 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,329 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,329 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,356 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,441 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,441 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,475 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,559 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,560 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,588 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,674 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,675 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,704 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,791 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,791 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,820 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:14,907 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:14,907 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:14,933 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,011 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,011 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,036 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,118 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,119 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/resnet.py\", line 66, in forward\n",
      "    out = F.relu(self.bn2(self.conv2(out)))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 19.75 MiB is free. Process 87262 has 1.17 GiB memory in use. Process 87261 has 5.39 GiB memory in use. Process 4013071 has 2.70 GiB memory in use. Including non-PyTorch memory, this process has 13.86 GiB memory in use. Of the allocated memory 12.21 GiB is allocated by PyTorch, and 510.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,199 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,233 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,233 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,263 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,300 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,300 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,334 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,372 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,373 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,420 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,458 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,458 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,491 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,530 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,531 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,563 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,599 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,600 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,651 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,681 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,681 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,712 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,751 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,751 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,783 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,821 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,821 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,874 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,911 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,911 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:15,940 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:15,978 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:15,979 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:16,010 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:16,047 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:16,048 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:16,100 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:16,139 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:16,139 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:16,169 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:16,201 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:16,202 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "2024-03-25 13:21:16,413 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=_splitnn_task_train_, id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "2024-03-25 13:21:16,417 - SplitNNController - ERROR - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: Ignore the task <nvflare.apis.controller_spec.Task object at 0x7f705bc3cdc0> result. Train result error code: EXECUTION_EXCEPTION\n",
      "2024-03-25 13:21:16,419 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: finished processing client result by splitnn_ctl\n",
      "2024-03-25 13:21:16,421 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:bcbde2b2-b4e5-41c4-85b5-cfb8545363d5\n",
      "0\n",
      "2024-03-25 13:21:16,233 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:16,270 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:16,271 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "0\n",
      "2024-03-25 13:21:16,320 - Cell - INFO - broadcast: channel='aux_communication', topic='_splitnn_task_valid_label_step_', targets=['site-2.simulate_job'], timeout=60.0\n",
      "1\n",
      "2024-03-25 13:21:16,357 - AuxRunner - ERROR - [identity=site-2, run=simulate_job, peer=site-1, peer_run=simulate_job]: processing error in message handling\n",
      "2024-03-25 13:21:16,358 - AuxRunner - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/private/aux_runner.py\", line 109, in _process_request\n",
      "    reply = handler_f(topic=topic, request=request, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 453, in _aux_val_label_side\n",
      "    self._val_step_label_side(batch_indices=batch_indices, activations=fobs.loads(activations), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 303, in _val_step_label_side\n",
      "    pred = self.model.forward(activations)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 61, in forward\n",
      "    x = self.split_forward(x)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/split_nn.py\", line 27, in forward\n",
      "    return x.reshape(32, 1024, 8, 8)\n",
      "RuntimeError: shape '[32, 1024, 8, 8]' is invalid for input of size 2031616\n",
      "\n",
      "2024-03-25 13:21:16,400 - SplitNNLearnerExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: learner execute exception: RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2024-03-25 13:21:16,401 - SplitNNLearnerExecutor - ERROR - Traceback (most recent call last):\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/nvflare/app_common/executors/splitnn_learner_executor.py\", line 79, in execute\n",
      "    return self.learner.train(shareable=shareable, fl_ctx=fl_ctx, abort_signal=abort_signal)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 543, in train\n",
      "    activations = self._train_forward_backward_data_side(fl_ctx, gradients)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 356, in _train_forward_backward_data_side\n",
      "    result_backward = self._backward_data_side(gradient, fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 469, in _backward_data_side\n",
      "    self._backward_step_data_side(gradient=fobs.loads(gradient), fl_ctx=fl_ctx)\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/examples/advanced/vertical_federated_learning/cifar10-split-res/src/splitnn/cifar10_learner_splitnn.py\", line 343, in _backward_step_data_side\n",
      "    self.train_activations.backward(gradient=gradient.reshape(self.train_activations.shape))\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/swarm/Desktop/NVFlare/NVFlare2/NVFlare/nvflare-env/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "2024-03-25 13:21:16,402 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: finished processing task\n",
      "2024-03-25 13:21:16,403 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: try #1: sending task result to server\n",
      "2024-03-25 13:21:16,403 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: checking task ...\n",
      "2024-03-25 13:21:16,403 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2024-03-25 13:21:16,410 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: start to send task result to server\n",
      "2024-03-25 13:21:16,410 - FederatedClient - INFO - Starting to push execute result.\n",
      "2024-03-25 13:21:16,424 - Communicator - INFO -  SubmitUpdate size: 555B (555 Bytes). time: 0.013860 seconds\n",
      "2024-03-25 13:21:16,424 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=_splitnn_task_train_, task_id=bcbde2b2-b4e5-41c4-85b5-cfb8545363d5]: task result sent to server\n",
      "2024-03-25 13:21:16,425 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 2 task_processed: True\n",
      "2024-03-25 13:21:16,478 - SplitNNController - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: task _splitnn_task_train_ exit with status TaskCompletionStatus.OK\n",
      "2024-03-25 13:21:16,643 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: Workflow: splitnn_ctl finalizing ...\n",
      "2024-03-25 13:21:16,681 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:21:16,683 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:21:16,685 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: END_RUN fired\n",
      "2024-03-25 13:21:16,687 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: Server runner finished.\n",
      "2024-03-25 13:21:16,772 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-2, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2024-03-25 13:21:16,775 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2024-03-25 13:21:16,828 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 Not Connected] is closed PID: 4012948\n",
      "2024-03-25 13:21:16,779 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2024-03-25 13:21:16,780 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2024-03-25 13:21:16,780 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: started end-run events sequence\n",
      "2024-03-25 13:21:16,780 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:21:16,781 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:21:16,781 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: END_RUN fired\n",
      "2024-03-25 13:21:16,781 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2024-03-25 13:21:16,826 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-2 \n",
      "2024-03-25 13:21:16,828 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 4013072\n",
      "2024-03-25 13:21:18,429 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl, peer=site-1, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2024-03-25 13:21:18,432 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2024-03-25 13:21:18,440 - FederatedClient - INFO - Shutting down client run: site-1\n",
      "2024-03-25 13:21:18,441 - FederatedClient - INFO - Shutting down client run: site-2\n",
      "2024-03-25 13:21:18,442 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=splitnn_ctl]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2024-03-25 13:21:18,444 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00005 Not Connected] is closed PID: 4012948\n",
      "2024-03-25 13:21:18,436 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2024-03-25 13:21:18,437 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2024-03-25 13:21:18,437 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: started end-run events sequence\n",
      "2024-03-25 13:21:18,438 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: ABOUT_TO_END_RUN fired\n",
      "2024-03-25 13:21:18,438 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: Firing CHECK_END_RUN_READINESS ...\n",
      "2024-03-25 13:21:18,438 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: END_RUN fired\n",
      "2024-03-25 13:21:18,438 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2024-03-25 13:21:18,439 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-1 \n",
      "2024-03-25 13:21:18,443 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 4013071\n",
      "2024-03-25 13:21:18,659 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2024-03-25 13:21:18,910 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 58939 shutdown!\n",
      "2024-03-25 13:21:18,912 - SimulatorServer - INFO - shutting down server\n",
      "2024-03-25 13:21:18,914 - SimulatorServer - INFO - canceling sync locks\n",
      "2024-03-25 13:21:18,915 - SimulatorServer - INFO - server off\n",
      "2024-03-25 13:21:22,362 - MPM - INFO - MPM: Good Bye!\n",
      "INFO:SimulatorRunner:return_code from process.exitcode: 0\n",
      "Simulator finished with run_status 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nvflare import SimulatorRunner\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=f\"jobs/cifar10_splitnn\",\n",
    "    workspace=\"/tmp/nvflare/cifar10_splitnn\",\n",
    "    n_clients=2,\n",
    "    threads=2\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9ee2-e993-442d-a525-d2baf92af539",
   "metadata": {},
   "source": [
    "The site containing the labels can compute accuracy and losses, which can be visualized in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6814434-4e6d-4460-b480-709cb3e77cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b8f2edc253ba3c92\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b8f2edc253ba3c92\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir /tmp/nvflare/cifar10_splitnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463c671-af33-45e5-a514-df5c759b35f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare-env",
   "language": "python",
   "name": "nvflare-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
